# Model configuration
model:
  in_channels: 2
  base_channels: 128
  latent_dim: 64
  c_mults: [[1, 1], [1, 2], [2, 4], [4, 8], [8, 16]]
  strides: [2, 4, 4, 8, 8]

# Training configuration
training:
  batch_size: 4
  num_epochs: 500
  learning_rate: 1.0e-4  # Reduced from 2e-4 for stability
  weight_decay: 1.0e-4

  # Loss weights
  kl_weight: 1.0e-4  # Increased from 1e-6 for better balance
  kl_anneal_epochs: 50  # Increased from 2 for gradual annealing

  # Reconstruction loss scaling (to balance with KL)
  recon_loss_scale: 1.0  # Can adjust if needed

  # Optimizer
  adam_betas: [0.9, 0.999]
  gradient_clip: 5.0  # Increased from 1.0 for less restrictive clipping

# Data configuration
data:
  train_dir: "./data/vocal_dataset"
  target_length: 88200  # 2 seconds at 44.1kHz (matches model design)
  target_sr: 44100
  train_split: 0.9 # 90% train, 10% validation
  num_workers: 2

# Checkpoint configuration
checkpointing:
  save_dir: "./checkpoints"
  save_every_n_epochs: 2
  keep_last_n: 3 # Keep only last 3 checkpoints to save space

# Logging configuration
logging:
  wandb_project: "LyraCodec"
  log_every_n_steps: 10
  save_audio_every_n_epochs: 10 # Save audio samples to WandB
  num_audio_samples: 4 # Number of audio samples to log

# System
system:
  device: "cuda" # or "cpu"
  mixed_precision: true # Use automatic mixed precision
